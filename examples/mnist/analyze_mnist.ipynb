{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence Analysis on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "from typing import Tuple\n",
    "from typing import Literal\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from kronfluence.analyzer import Analyzer, prepare_model\n",
    "from kronfluence.arguments import FactorArguments, ScoreArguments\n",
    "from kronfluence.task import Task\n",
    "from kronfluence.utils.common.factor_arguments import all_low_precision_factor_arguments\n",
    "from kronfluence.utils.common.score_arguments import all_low_precision_score_arguments\n",
    "from kronfluence.utils.dataset import DataLoaderKwargs\n",
    "from examples.mnist.pipeline import get_mnist_dataset, construct_mnist_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_TYPE = Tuple[torch.Tensor, torch.Tensor]\n",
    "\n",
    "\n",
    "class ClassificationTask(Task):\n",
    "    def compute_train_loss(\n",
    "        self,\n",
    "        batch: BATCH_TYPE,\n",
    "        model: nn.Module,\n",
    "        sample: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        inputs, labels = batch\n",
    "        logits = model(inputs)\n",
    "        if not sample:\n",
    "            return F.cross_entropy(logits, labels, reduction=\"sum\")\n",
    "        with torch.no_grad():\n",
    "            probs = torch.nn.functional.softmax(logits.detach(), dim=-1)\n",
    "            sampled_labels = torch.multinomial(\n",
    "                probs,\n",
    "                num_samples=1,\n",
    "            ).flatten()\n",
    "        return F.cross_entropy(logits, sampled_labels, reduction=\"sum\")\n",
    "\n",
    "    def compute_measurement(\n",
    "        self,\n",
    "        batch: BATCH_TYPE,\n",
    "        model: nn.Module,\n",
    "    ) -> torch.Tensor:\n",
    "        # Copied from: https://github.com/MadryLab/trak/blob/main/trak/modelout_functions.py. Returns the margin between the correct logit and the second most likely prediction\n",
    "        inputs, labels = batch\n",
    "        logits = model(inputs)\n",
    "\n",
    "        # Get correct logit values\n",
    "        bindex = torch.arange(logits.shape[0]).to(device=logits.device, non_blocking=False)\n",
    "        logits_correct = logits[bindex, labels]\n",
    "\n",
    "        # Get the other logits, and take the softmax of them\n",
    "        cloned_logits = logits.clone()\n",
    "        cloned_logits[bindex, labels] = torch.tensor(-torch.inf, device=logits.device, dtype=logits.dtype)\n",
    "        maximum_non_correct_logits = cloned_logits.logsumexp(dim=-1)\n",
    "\n",
    "        # Look at the  margin, the difference between the correct logits and the (soft) maximum non-correctl logits\n",
    "        margins = logits_correct - maximum_non_correct_logits\n",
    "        return -margins.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/h/maxk/kronfluence/data\"\n",
    "output_dir = \"/h/maxk/kronfluence/examples/mnist/influence_results\"\n",
    "model_path = \"/h/maxk/kronfluenFce/ checkpoints/model.pth\"\n",
    "factor_strategy: Literal[\"identity\", \"diagonal\", \"kfac\", \"ekfac\"] = \"ekfac\"  # TODO: Add typesc for the\n",
    "profile_computations = False\n",
    "use_half_precision = False\n",
    "query_batch_size = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset.\n",
    "train_dataset = get_mnist_dataset(split=\"eval_train\", dataset_dir=dataset_dir, in_memory=False)\n",
    "eval_dataset = get_mnist_dataset(split=\"test\", dataset_dir=output_dir, in_memory=False)\n",
    "\n",
    "# Prepare the trained model.\n",
    "model = construct_mnist_classifier()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Define task and prepare model.\n",
    "task = ClassificationTask()\n",
    "model = prepare_model(model, task)\n",
    "\n",
    "analyzer = Analyzer(\n",
    "    analysis_name=\"mnist\",\n",
    "    model=model,\n",
    "    task=task,\n",
    "    output_dir=output_dir,\n",
    "    profile=profile_computations,\n",
    ")\n",
    "# Configure parameters for DataLoader.\n",
    "dataloader_kwargs = DataLoaderKwargs(num_workers=4)\n",
    "analyzer.set_dataloader_kwargs(dataloader_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute influence factors.\n",
    "factors_name = factor_strategy\n",
    "factor_args = FactorArguments(strategy=factor_strategy)\n",
    "if use_half_precision:\n",
    "    factor_args = all_low_precision_factor_arguments(strategy=factor_strategy, dtype=torch.bfloat16)\n",
    "    factors_name += \"_half\"\n",
    "\n",
    "analyzer.fit_all_factors(\n",
    "    factors_name=factors_name,\n",
    "    factor_args=factor_args,\n",
    "    dataset=train_dataset,\n",
    "    per_device_batch_size=None,\n",
    "    overwrite_output_dir=False,\n",
    ")\n",
    "\n",
    "# Compute pairwise scores.\n",
    "score_args = ScoreArguments()\n",
    "scores_name = factor_args.strategy\n",
    "\n",
    "if use_half_precision:\n",
    "    score_args = all_low_precision_score_arguments(dtype=torch.bfloat16)\n",
    "    scores_name += \"_half\"\n",
    "\n",
    "analyzer.compute_pairwise_scores(\n",
    "    scores_name=scores_name,\n",
    "    score_args=score_args,\n",
    "    factors_name=factors_name,\n",
    "    query_dataset=eval_dataset,\n",
    "    query_indices=list(range(2000)),\n",
    "    train_dataset=train_dataset,\n",
    "    per_device_query_batch_size=args.query_batch_size,\n",
    "    overwrite_output_dir=False,\n",
    ")\n",
    "scores = analyzer.load_pairwise_scores(scores_name)[\"all_modules\"]\n",
    "logging.info(f\"Scores shape: {scores.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
